# -*- coding: utf-8 -*-
"""preferences_generator.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19C5AhPH3JqFLgd6ZZFGLW5gpa-YwBSIx

# Packages
"""

!pip install PySastrawi

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from sklearn.feature_extraction.text import TfidfVectorizer
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory

"""# Corpus"""

file_paths = {
    "all_locations": "/content/all_locations.csv",
    # "bandung": "/content/bandung.csv",
    # "banjarbaru": "/content/banjarbaru.csv",
    # "bengkulu": "/content/bengkulu.csv",
    # "denpasar": "/content/denpasar.csv",
    # "jakarta": "/content/jakarta.csv",
    # "jayapura": "/content/jayapura.csv",
    # "maluku": "/content/maluku.csv",
    # "semarang": "/content/semarang.csv",
    # "surabaya": "/content/surabaya.csv",
    # "yogyakarta": "/content/yogyakarta.csv"
}

datasets = {key: pd.read_csv(path) for key, path in file_paths.items()}

"""# Data preprocessing

## Stemming
"""

stemmed_documents = []

def stemming(city, column):
  corpus = datasets[city][column]

  # stop words with PySastrawi
  stopword_factory = StopWordRemoverFactory()
  remover = stopword_factory.create_stop_word_remover()

  # stemming with PySastrawi
  stemming_factory = StemmerFactory()
  stemmer = stemming_factory.create_stemmer()

  # store in stemmed documents
  for doc in corpus:
    stemmed_doc = [stemmer.stem(word) for word in doc.split()]
    cleaned_text = remover.remove(' '.join(stemmed_doc))
    stemmed_documents.append(cleaned_text)

  return stemmed_documents

"""## Tokenization"""

# Parameters
num_words = None
oov_tok = "<OOV>"
lower=True
char_level = False
filters='!"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n'

def tokenizer(city, column, save, stemmed_documents):
  # Define the tokenizer
  tokenizer = Tokenizer(num_words=num_words,
                        filters=filters,
                        oov_token=oov_tok,
                        lower=lower,
                        char_level=char_level)

  # Fit tokenizer on texts
  tokenizer.fit_on_texts(stemmed_documents)

  # Word index
  word_index = tokenizer.word_index
  word_index_df = pd.DataFrame(list(word_index.items()), columns=['word', 'index'])

  # Word counts
  word_counts = tokenizer.word_counts
  word_counts_df = pd.DataFrame(list(word_counts.items()), columns=['word', 'count']).sort_values(by='count', ascending=False)

  # Save the data frame
  print(word_index_df)
  print(word_counts_df)

"""## Preprocessing"""

def preprocessing(city, column, save):
  stemming(city, column)
  tokenizer(city, column, save, stemmed_documents)

preprocessing("all_locations", "metadata", True)

"""# Word Embedding

# TF-IDF

### TF-IDF in One Corpus
"""

from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd

def tfidf_corpus(stemmed_documents, save):
    # Initialize vectorizer
    vectorizer = TfidfVectorizer()

    # Fit vectorizer on all documents
    response = vectorizer.fit_transform(stemmed_documents)

    # Get feature names (words)
    feature_names = vectorizer.get_feature_names_out()

    # Calculate TF-IDF values
    tfidf_values = response.sum(axis=0).A1

    # Save or return DataFrame
    if save is True:
        df = pd.DataFrame({'word': feature_names, 'tfidf': tfidf_values})
        df = df.sort_values(by='tfidf', ascending=False)
        df.to_excel('tfidf_corpus_keywords.xlsx', index=False)
        return df
    else:
        return None

tfidf_corpus(stemmed_documents, True)

"""### TF-IDF in All Documents"""

def tfidf_docs(stemmed_documents, save):
    # Initialize vectorizer
    vectorizer = TfidfVectorizer()

    # Fit vectorizer on each document
    response = vectorizer.fit_transform(stemmed_documents)

    if save is True:
        feature_names = vectorizer.get_feature_names_out()
        tfidf_values = response.todense().tolist()
        df = pd.DataFrame(tfidf_values, columns=feature_names)
        df = df.transpose()
        df.to_excel('tfidf_docs_keywords.xlsx', index=True)
        return df
    else:
        return None

tfidf_docs(stemmed_documents, True)