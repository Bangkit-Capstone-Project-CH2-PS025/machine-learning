{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpW-DRr6bsJj"
      },
      "source": [
        "# Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQmf_d3ZaA21",
        "outputId": "5b54fd96-0b98-4e61-a8da-a073b9c87c20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n",
            "Collecting PySastrawi\n",
            "  Downloading PySastrawi-1.2.0-py2.py3-none-any.whl (210 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.6/210.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PySastrawi\n",
            "Successfully installed PySastrawi-1.2.0\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2023.11.17)\n"
          ]
        }
      ],
      "source": [
        "!pip install gensim\n",
        "!pip install PySastrawi\n",
        "!pip install requests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jCQ7yjBabiB"
      },
      "source": [
        "# Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "o2vWasTIaIxw",
        "outputId": "dd2b32fd-d595-496e-8e33-326ead06810d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import gensim\n",
        "import nltk\n",
        "from collections import OrderedDict\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
        "from sklearn.metrics.pairwise import cosine_distances\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qpZ_9Mgbn_l"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BXuYp8CwatGo"
      },
      "outputs": [],
      "source": [
        "file_paths = {\n",
        "    \"bandung\": \"/content/bandung.csv\",\n",
        "}\n",
        "\n",
        "datasets = {key: pd.read_csv(path) for key, path in file_paths.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mynZtrP0b8bJ"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TextPreprocessing():\n",
        "  def __init__(self, data, city):\n",
        "    self.data = data\n",
        "    self.city = city\n",
        "    self.column = 'metadata'\n",
        "\n",
        "  def stemming(self):\n",
        "    stemmed_documents = []\n",
        "    corpus = self.data[self.city][self.column]\n",
        "\n",
        "    # stop words with PySastrawi\n",
        "    stopword_factory = StopWordRemoverFactory()\n",
        "    remover = stopword_factory.create_stop_word_remover()\n",
        "\n",
        "    # stemming with PySastrawi\n",
        "    stemming_factory = StemmerFactory()\n",
        "    stemmer = stemming_factory.create_stemmer()\n",
        "\n",
        "    # store in stemmed documents\n",
        "    for doc in corpus:\n",
        "      stemmed_doc = [stemmer.stem(word) for word in doc.split()]\n",
        "      cleaned_text = remover.remove(' '.join(stemmed_doc))\n",
        "      stemmed_documents.append(cleaned_text)\n",
        "\n",
        "    return stemmed_documents\n",
        "\n",
        "  def tokenizer(self, stemmed_documents):\n",
        "    num_words = None\n",
        "    oov_tok = \"<OOV>\"\n",
        "    lower=True\n",
        "    char_level = False\n",
        "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
        "\n",
        "    # Define the tokenizer\n",
        "    tokenizer = Tokenizer(num_words=num_words,\n",
        "                          filters=filters,\n",
        "                          oov_token=oov_tok,\n",
        "                          lower=lower,\n",
        "                          char_level=char_level)\n",
        "\n",
        "    # Fit tokenizer on texts\n",
        "    tokenizer.fit_on_texts(stemmed_documents)\n",
        "\n",
        "    tokenized_texts = tokenizer.texts_to_sequences(stemmed_documents)\n",
        "\n",
        "    tokenized_strings = tokenizer.sequences_to_texts(tokenized_texts)\n",
        "\n",
        "    self.data[self.city]['metadata_tokenized'] = tokenized_strings\n",
        "\n",
        "    return datasets, tokenized_strings"
      ],
      "metadata": {
        "id": "yXDhKINoeIrh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKryZG4-iq10"
      },
      "source": [
        "# User Input"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class User():\n",
        "  def __init__(self):\n",
        "    self.city = None\n",
        "    self.budget = None\n",
        "    self.duration = None\n",
        "    self.user_preferences = None\n",
        "    self.combined_list = None\n",
        "\n",
        "  def input(self, city, budget, duration, user_preferences_1, user_preferences_2=None):\n",
        "    self.city = str(city)\n",
        "    self.budget = float(budget)\n",
        "    self.duration = int(duration)\n",
        "    self.user_preferences = [user_preferences_1]\n",
        "\n",
        "    if user_preferences_2 is not None:\n",
        "      self.user_preferences.append(user_preferences_2)\n",
        "\n",
        "    self.combined_list = datasets[city]['metadata_tokenized'].tolist()\n",
        "    return self.combined_list, self.user_preferences, self.duration, self.budget"
      ],
      "metadata": {
        "id": "0ORbg9FYq061"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZbltZ7Vcoci"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-Bk5diycrXg"
      },
      "source": [
        "## Word2vec"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SumPreferences():\n",
        "  def __init__(self):\n",
        "    self.user_preferences = None\n",
        "    self.dataset = None\n",
        "    self.vector_size = 100\n",
        "    self.window = 500\n",
        "    self.min_count = 2\n",
        "    self.workers = 100\n",
        "\n",
        "  def fit(self, dataset, user_preferences):\n",
        "    self.user_preferences = user_preferences\n",
        "    self.dataset = dataset\n",
        "\n",
        "    if len(self.user_preferences) != 1:\n",
        "      tokenized_data = [gensim.utils.simple_preprocess(sentence) for sentence in dataset]\n",
        "      model = gensim.models.Word2Vec(tokenized_data, vector_size=self.vector_size, window=self.window, min_count=self.min_count, workers=self.workers)\n",
        "      similarity = model.wv.similarity(w1=self.user_preferences[0], w2=self.user_preferences[1])\n",
        "      similar_words = model.wv.most_similar(positive=[self.user_preferences[0], self.user_preferences[1]])\n",
        "      user_preferences.append(similar_words[0][0])\n",
        "    else:\n",
        "      pass\n",
        "    return user_preferences[2]"
      ],
      "metadata": {
        "id": "Xo8t5tRDuoM9"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recommender System"
      ],
      "metadata": {
        "id": "0STzYwc6ZX6A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RecommenderSystem:\n",
        "  def __init__(self, data, city, metadata_tokenized):\n",
        "    self.data = data\n",
        "    self.city = data[city]\n",
        "    self.metadata_tokenized = metadata_tokenized\n",
        "    self.vectorizer = None\n",
        "    self.documents = None\n",
        "    self.recommendation_result = None\n",
        "\n",
        "  def fit(self):\n",
        "    # encode with TF-IDF\n",
        "    self.vectorizer = TfidfVectorizer()\n",
        "    self.documents = self.vectorizer.fit_transform(self.metadata_tokenized)\n",
        "\n",
        "  def recommend(self, user_preferences, top_recommend=10):\n",
        "    self.list_appended = []\n",
        "\n",
        "    for preference in user_preferences:\n",
        "      preference_vector = self.vectorizer.transform([preference])\n",
        "      distance = cosine_distances(preference_vector, self.documents)\n",
        "      distance_sort = distance.argsort()[0, 0:top_recommend]\n",
        "      self.list_appended.append(distance_sort)\n",
        "    return self.list_appended\n",
        "\n",
        "  def postprocessing(self):\n",
        "    combined = list(OrderedDict.fromkeys(np.concatenate(self.list_appended)))\n",
        "    self.recommendation_result = self.city.iloc[combined]\n",
        "    return self.recommendation_result"
      ],
      "metadata": {
        "id": "UkaGrhkSya2W"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filtering"
      ],
      "metadata": {
        "id": "QEKu2mWNILze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Filtering:\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "        self.recommendation_budget = None\n",
        "        self.recommendation_distance = None\n",
        "\n",
        "    def budgeting(self, budget):\n",
        "        self.total_price = 0\n",
        "        self.selected_rows = []\n",
        "\n",
        "        for index, row in self.data.iterrows():\n",
        "            if self.total_price + row['price'] <= budget:\n",
        "                self.total_price += row['price']\n",
        "                self.selected_rows.append(row)\n",
        "\n",
        "        self.recommendation_budget = pd.DataFrame(self.selected_rows)\n",
        "        return self.recommendation_budget\n",
        "\n",
        "    def haversine_distance(self, lat1, lon1, lat2, lon2):\n",
        "        R = 6371\n",
        "        dlat = np.radians(lat2 - lat1)\n",
        "        dlon = np.radians(lon2 - lon1)\n",
        "        a = np.sin(dlat/2) * np.sin(dlat/2) + np.cos(np.radians(lat1)) * np.cos(np.radians(lat2)) * np.sin(dlon/2) * np.sin(dlon/2)\n",
        "        c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
        "        distance = R * c\n",
        "        return distance\n",
        "\n",
        "    def nearest_neighbor(self, points, start_lat, start_lon):\n",
        "        self.route = []\n",
        "        self.total_distance = None\n",
        "        start_point = np.array([[start_lat, start_lon]])\n",
        "        points = np.concatenate((points, start_point), axis=0)\n",
        "\n",
        "        n = len(points)\n",
        "        visited = np.zeros(n, dtype=bool)\n",
        "        visited[-1] = True\n",
        "        route = [n - 1]\n",
        "        total_dist = 0\n",
        "\n",
        "        for i in range(n - 1):\n",
        "            last_point = route[-1]\n",
        "            min_dist = float('inf')\n",
        "            nearest_point = None\n",
        "            for j in range(n):\n",
        "                if not visited[j] and j != last_point:\n",
        "                    dist = self.haversine_distance(points[last_point][0], points[last_point][1], points[j][0], points[j][1])\n",
        "                    if dist < min_dist:\n",
        "                        min_dist = dist\n",
        "                        nearest_point = j\n",
        "            visited[nearest_point] = True\n",
        "            route.append(nearest_point)\n",
        "            total_dist += min_dist\n",
        "\n",
        "        route.remove(n - 1)\n",
        "        total_dist += self.haversine_distance(points[route[-1]][0], points[route[-1]][1], points[route[0]][0], points[route[0]][1])\n",
        "\n",
        "        return route, total_dist\n",
        "\n",
        "    def distance(self, start_lat, start_long):\n",
        "        self.start_lat = start_lat\n",
        "        self.start_long = start_long\n",
        "        self.coordinates = self.recommendation_budget[['lat', 'long']].values\n",
        "\n",
        "        route, total_distance = self.nearest_neighbor(self.coordinates, self.start_lat, self.start_long)\n",
        "        self.route = route\n",
        "        self.total_distance = total_distance\n",
        "        corresponding_indices = [self.recommendation_budget.iloc[i]['id'] for i in self.route]\n",
        "        self.recommendation_distance = self.recommendation_budget.iloc[route]\n",
        "        return self.route, self.total_distance, self.recommendation_distance"
      ],
      "metadata": {
        "id": "AwH4NovEOi04"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Carbon Footprint"
      ],
      "metadata": {
        "id": "ngCiw9PSo8RM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CarbonFootprint():\n",
        "    def __init__(self):\n",
        "        self.co2_emissions = None\n",
        "\n",
        "    def car(self):\n",
        "        emission_factors = 0.1639\n",
        "        return emission_factors\n",
        "\n",
        "    def bus(self):\n",
        "        emission_factors = 0.1022\n",
        "        return emission_factors\n",
        "\n",
        "    def motorbike(self):\n",
        "        emission_factors = 0.1133\n",
        "        return emission_factors\n",
        "\n",
        "    # select type of vehicle\n",
        "    def vehicle_type(self, vehicle_type):\n",
        "        if vehicle_type == \"car\":\n",
        "            return self.car()\n",
        "        elif vehicle_type == \"bus\":\n",
        "            return self.bus()\n",
        "        elif vehicle_type == \"motorbike\":\n",
        "            return self.motorbike()\n",
        "\n",
        "    # calculate\n",
        "    def calculate(self, vehicle_type, distance):\n",
        "        self.co2_emissions = self.vehicle_type(vehicle_type) * distance\n",
        "        return round(self.co2_emissions, 1)\n",
        "\n",
        "    def calculate_all(self, distance):\n",
        "      self.distance = distance\n",
        "      results = []\n",
        "      vehicle_type = ['car', 'bus', 'motorbike']\n",
        "      for vehicle in vehicle_type:\n",
        "        result = self.calculate(vehicle, self.distance)\n",
        "        results.append(f\"{result} kg CO2\")\n",
        "      return results"
      ],
      "metadata": {
        "id": "VWRVEq2fQa40"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pipeline"
      ],
      "metadata": {
        "id": "cx3u_Iou48bX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_pipeline(datasets, city, budget, duration, user_preferences_1, user_preferences_2=None):\n",
        "  start_latitude = -3.801742\n",
        "  start_longitude = 102.226509\n",
        "\n",
        "  preprocess = TextPreprocessing(datasets, city)\n",
        "  stemmed_documents = preprocess.stemming()\n",
        "  datasets, tokenized_strings = preprocess.tokenizer(stemmed_documents)\n",
        "  # User Input\n",
        "  user = User()\n",
        "  combined, user_preferences, duration, budget = user.input(city, budget, duration, user_preferences_1, user_preferences_2)\n",
        "  # Preferences\n",
        "  preferences = SumPreferences()\n",
        "  user_preferences_new = preferences.fit(combined, user_preferences)\n",
        "  # Recommender System\n",
        "  recommender = RecommenderSystem(datasets, city, tokenized_strings)\n",
        "  recommender.fit()\n",
        "  recommender.recommend(user_preferences)\n",
        "  recommendation_result = recommender.postprocessing()\n",
        "  # Filtering\n",
        "  filter = Filtering(recommendation_result)\n",
        "  recommendation_budget = filter.budgeting(budget)\n",
        "  route, total_distance, recommendation_distance = filter.distance(start_latitude, start_longitude)\n",
        "  # Calculate Carbon Footprint\n",
        "  footprint = CarbonFootprint()\n",
        "  results = footprint.calculate_all(total_distance)\n",
        "  return results\n",
        "\n",
        "results = make_pipeline(datasets, \"bandung\", 100000, 1, \"wisata\", \"bandung\")\n",
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLTFfqSHsLix",
        "outputId": "a2151f1c-1c0d-4aaf-9578-2aa99c93c085"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['132.6 kg CO2', '82.7 kg CO2', '91.6 kg CO2']"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}